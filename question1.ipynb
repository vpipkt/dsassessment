{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Manipulation with Spark SQL\n",
    "\n",
    "Prepared by Jason T. Brown\n",
    "\n",
    "2020 Aug 29\n",
    "\n",
    "Read JSON data into Spark SQL, conduct some analysis on the frequency of observations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure\n",
    "\n",
    "Default local spark context settings used for portability. The data is small enough for the processing to complete very quickly on this.\n",
    "\n",
    "This should be able to run in any Python 3 environment with a `pip` installed Spark 2.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL\n",
    "\n",
    "Because the dataset is small we will read in memory then convert into a Spark RDD, then a Spark SQL DataFrame. A bit unusual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/classification/DM-classification.json') as f:\n",
    "    data_and_schema = json.load(f)\n",
    "    data = data_and_schema['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field names we want to use is not reflected in the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fields': [{'name': 'index', 'type': 'integer'},\n",
       "  {'name': 'content', 'type': 'string'},\n",
       "  {'name': 'label', 'type': 'integer'},\n",
       "  {'name': 'label_1', 'type': 'string'},\n",
       "  {'name': 'label_2', 'type': 'string'},\n",
       "  {'name': 'label_3', 'type': 'number'},\n",
       "  {'name': 'label_4', 'type': 'datetime'}],\n",
       " 'primaryKey': ['index'],\n",
       " 'pandas_version': '0.20.0'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_and_schema['schema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('content', StringType()),\n",
    "    StructField('label', IntegerType()),\n",
    "    StructField('label_1', StringType()), #size\n",
    "    StructField('label_2', StringType()), # usage\n",
    "    StructField('label_3', DoubleType()), # effect \n",
    "    StructField('label_4', TimestampType()) # date\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(spark.sparkContext.parallelize(data), \n",
    "                     schema=schema,\n",
    "                     mode='FAILFAST') \\\n",
    "        .withColumnRenamed('label_1', 'size') \\\n",
    "        .withColumnRenamed('label_2', 'usage') \\\n",
    "        .withColumnRenamed('label_3', 'effect') \\\n",
    "        .withColumnRenamed('label_4', 'date') \n",
    "\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------+------------+-------------------+\n",
      "|             content|label|  size|   usage|      effect|               date|\n",
      "+--------------------+-----+------+--------+------------+-------------------+\n",
      "|The battery is co...|    0| small|separate|0.7155163569|2015-06-05 18:41:08|\n",
      "|What a big waste ...|    0|medium|conected| 0.858630808|2016-10-29 12:12:46|\n",
      "|Don't waste your ...|    0| large|conected|0.2040485858|2016-04-29 14:44:31|\n",
      "|Great sound and s...|    1| large|separate| 0.332641236|2017-12-26 13:25:48|\n",
      "|Really pleased wi...|    1|medium|conected| 0.887390017|2016-04-30 00:01:08|\n",
      "|One of my favorit...|    1| large|conected|0.2305351126|2016-04-30 17:29:03|\n",
      "|best bluetooth on...|    1|medium|conected|0.4549175852|2017-04-24 04:26:54|\n",
      "|Authentic leather...|    1| large|conected|0.3198441525|2015-12-16 22:03:11|\n",
      "|I was very excite...|    1|medium|conected| 0.835863266|2015-05-19 01:34:19|\n",
      "|Do not make the s...|    0| small|conected|0.1442302304|2015-02-03 03:31:18|\n",
      "+--------------------+-----+------+--------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM df LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the window: groups by `size` and orders by date. Chooses \"next\" row only.  This lets us calculate the difference between `date` in consecutive records, in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Window.partitionBy('size') \\\n",
    "           .orderBy('date') \\\n",
    "           .rowsBetween(0, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+----------+-------------------+\n",
      "|  size|               date|date_gap_s|           gap_days|\n",
      "+------+-------------------+----------+-------------------+\n",
      "|medium|2015-01-01 23:54:55|    188222|   2.17849537037037|\n",
      "|medium|2015-01-04 04:11:57|    818339|  9.471516203703706|\n",
      "|medium|2015-01-13 15:30:56|    113598| 1.3147916666666668|\n",
      "|medium|2015-01-14 23:04:14|     75788|  0.877175925925926|\n",
      "|medium|2015-01-15 20:07:22|     37015|0.42841435185185184|\n",
      "|medium|2015-01-16 06:24:17|    144718|  1.674976851851852|\n",
      "|medium|2015-01-17 22:36:15|    874460| 10.121064814814815|\n",
      "|medium|2015-01-28 01:30:35|    232926| 2.6959027777777775|\n",
      "|medium|2015-01-30 18:12:41|     92254| 1.0677546296296296|\n",
      "|medium|2015-01-31 19:50:15|    155257| 1.7969560185185185|\n",
      "|medium|2015-02-02 14:57:52|     50275| 0.5818865740740741|\n",
      "|medium|2015-02-03 04:55:47|    303770|  3.515856481481481|\n",
      "|medium|2015-02-06 17:18:37|    799491|  9.253368055555557|\n",
      "|medium|2015-02-15 23:23:28|    365441|  4.229641203703704|\n",
      "|medium|2015-02-20 04:54:09|    129240| 1.4958333333333333|\n",
      "|medium|2015-02-21 16:48:09|    253796|  2.937453703703704|\n",
      "|medium|2015-02-24 15:18:05|    127099| 1.4710532407407408|\n",
      "|medium|2015-02-26 02:36:24|    149793| 1.7337152777777776|\n",
      "|medium|2015-02-27 20:12:57|    334307| 3.8692939814814817|\n",
      "|medium|2015-03-03 17:04:44|    857500|  9.924768518518517|\n",
      "+------+-------------------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_gap_df = df.select(\n",
    "    'size', 'date', \n",
    "     (F.max(F.unix_timestamp(df.date)).over(ws) -\n",
    "      F.min(F.unix_timestamp(df.date)).over(ws)).alias('date_gap_s'))\n",
    "\n",
    "date_gap_df.withColumn('gap_days', F.col('date_gap_s') / 24 / 60 / 60).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+----------+------------------+\n",
      "| size|               date|date_gap_s|          gap_days|\n",
      "+-----+-------------------+----------+------------------+\n",
      "|large|2015-01-02 11:44:05|    114974|1.3307175925925925|\n",
      "|large|2015-01-03 19:40:19|    303618| 3.514097222222222|\n",
      "|large|2015-01-07 08:00:37|     58255|0.6742476851851851|\n",
      "|large|2015-01-08 00:11:32|    117074| 1.355023148148148|\n",
      "|large|2015-01-09 08:42:46|    466253| 5.396446759259259|\n",
      "|large|2015-01-14 18:13:39|     21786|0.2521527777777778|\n",
      "|large|2015-01-15 00:16:45|    383858| 4.442800925925926|\n",
      "|large|2015-01-19 10:54:23|    220700| 2.554398148148148|\n",
      "|large|2015-01-22 00:12:43|   1098646|12.715810185185184|\n",
      "|large|2015-02-03 17:23:29|    938456| 10.86175925925926|\n",
      "+-----+-------------------+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_gap_df.filter(F.col('size') == 'large') \\\n",
    "            .withColumn('gap_days', F.col('date_gap_s') / 24 / 60 / 60) \\\n",
    "            .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adding the `date_gap` in days to the `date` on the same row will get us the `date` on the following row.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data\n",
    "\n",
    "We can see the `small` size are the most frequent and `large` are the least frequent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|  size|  avg(date_gap_s)|\n",
      "+------+-----------------+\n",
      "|medium|         278014.0|\n",
      "| small|274956.4340175953|\n",
      "| large|    295512.553125|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_gap_df.groupby('size').agg(F.mean('date_gap_s')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save \n",
    "\n",
    "We will save results to CSV file.\n",
    "\n",
    "Because the data is small we will convert it to a pandas dataframe first. Pandas has a little nicer functionality for writing to CSV.\n",
    "\n",
    "I chose not to save out the conversion to days because saving the same data with different units is an anti-pattern. Downstream one of duplicates can become out of sync with each other can cause problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_gap_df.toPandas().to_csv('date_gap.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size,date,date_gap_s\n",
      "medium,2015-01-01 23:54:55,188222\n",
      "medium,2015-01-04 04:11:57,818339\n",
      "medium,2015-01-13 15:30:56,113598\n",
      "medium,2015-01-14 23:04:14,75788\n",
      "medium,2015-01-15 20:07:22,37015\n",
      "medium,2015-01-16 06:24:17,144718\n",
      "medium,2015-01-17 22:36:15,874460\n",
      "medium,2015-01-28 01:30:35,232926\n",
      "medium,2015-01-30 18:12:41,92254\n"
     ]
    }
   ],
   "source": [
    "!head date_gap.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
